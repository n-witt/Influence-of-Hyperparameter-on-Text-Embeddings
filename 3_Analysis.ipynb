{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "source": [
    "# Analysis\n",
    "The previous notebook carried out the brute-force hyperparameter search. In contrast, this notebook focuses on the analysis of these data. For that purpose it plots the averaged results of the exhaustive parameter search per corpus and sample size used for training, followed by a discussion of the results.\n",
    "The second part of this notebooks deals with the most successful configurations rather than averaged results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The `plot_results` function plots the results of the parameter search of one corpus at a given sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import product, cycle\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "def plot_correletaion_matrix(df, parameter, sample_size, title):\n",
    "    variables = list(parameter.keys())\n",
    "    num_vars = len(variables)\n",
    "    df = df.loc[df['sample_size'] == sample_size]\n",
    "\n",
    "    grouped = OrderedDict({})\n",
    "    for x, y in product(variables, variables):\n",
    "        grouped[(y, x)] = df['mean'].groupby([df[x], df[y]])\n",
    "\n",
    "    colors = ['tomato', 'black', 'green', 'c', 'gold', 'tan', 'royalblue', 'brown', 'purple']\n",
    "\n",
    "    # calc min and max values per row\n",
    "    min_max = defaultdict(lambda : {'min': 1, 'max': 0})\n",
    "    for (_, variable), series in grouped.items():\n",
    "        min_val = series.mean().min()\n",
    "        max_val = series.mean().max()\n",
    "        min_max[variable]['min'] = min_val if min_val < min_max[variable]['min'] else min_max[variable]['min']\n",
    "        min_max[variable]['max'] = max_val if max_val > min_max[variable]['max'] else min_max[variable]['max']\n",
    "\n",
    "    fig, ax = plt.subplots(num_vars, num_vars, figsize=(9.75, 9.75))\n",
    "\n",
    "    for p_coor, (param_names, series) in zip(product(range(num_vars), range(num_vars)), grouped.items()):\n",
    "        table = series.mean().unstack()\n",
    "        data = {row: np.array([(x, y) for x, y in series.items()]) for row, series in table.iterrows()}\n",
    "\n",
    "        # set log scale if config demands\n",
    "        if plot_params[param_names[0]]['scale'] == 'log':\n",
    "            ax[p_coor[0], p_coor[1]].set_xscale('log')\n",
    "\n",
    "        for c, (k, v) in zip(cycle(colors), data.items()):\n",
    "            axis = ax[p_coor[0], p_coor[1]]\n",
    "            axis.plot(v[:, 0], v[:, 1], c=c, label=k)\n",
    "            axis.set_ylim(\n",
    "                max(min_max[param_names[1]]['min'] * .95, 0) , \n",
    "                min(min_max[param_names[1]]['max'] * 1.05, 1)\n",
    "            )\n",
    "\n",
    "            # special treatment for diagonal items\n",
    "            if p_coor[0] == p_coor[1]:\n",
    "                axis.legend(loc='center right')\n",
    "                axis.tick_params(axis='both', which='both', left='off', bottom='off', right='off', top='off')\n",
    "                axis.grid('off')\n",
    "\n",
    "\n",
    "    for axis in ax[:-1, :].flatten():\n",
    "        axis.tick_params(\n",
    "            axis='x',\n",
    "            which='both',\n",
    "            top='off',\n",
    "            labelbottom='off')\n",
    "        axis.tick_params(\n",
    "            axis='y',\n",
    "            which='both',\n",
    "            right='off')\n",
    "\n",
    "    # horizontal labels\n",
    "    for (variable, value), axis in zip(parameter.items(), ax[-1, :].flatten()):\n",
    "        axis.xaxis.set_label_text(variable)\n",
    "        axis.xaxis.set_ticks(value)\n",
    "\n",
    "    # vertical labels\n",
    "    for v, axis in zip(variables, ax[:, 0].flatten()):\n",
    "        axis.yaxis.set_label_text(v)\n",
    "\n",
    "    # disable y-axis tick label for most subplots\n",
    "    for axis in ax[:, 1:].flatten():\n",
    "        axis.tick_params(\n",
    "            axis='y',\n",
    "            labelleft='off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    fig.text(.5, 1, title, horizontalalignment='center', fontsize=20) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the following cell the meta configuration (i.e. samples sizes and corpus configurations) from the previous notebook is loaded. Also the parameters for the plotting are set. **Note:** Enter the actual filename into the `results` tuple. Also, the `corpus` variable needs to be adapted to the corpus that you wish to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "experiment_data = None\n",
    "corpus = 'amazon'\n",
    "results = ('YYYYMMDD-HH:MM_gs_results.pkl') # enter actual filename here\n",
    "with open(results[-1], 'rb') as fh:\n",
    "    experiment_data = pickle.load(fh)\n",
    "    \n",
    "max_sample_size = max(experiment_data['sample_sizes'])\n",
    "\n",
    "experiment_data['param_labels'] = {\n",
    "    'trans__size' : r'd',\n",
    "    'trans__iter': r'epoch',\n",
    "    'trans__alpha': r'\\alpha',\n",
    "    'trans__negative': r'ns',\n",
    "    'trans__window': r'win',\n",
    "    'trans__hs': r'hs',\n",
    "    'trans__dm': r'arch',\n",
    "    'cls__k': r'k',        \n",
    "}\n",
    "\n",
    "plot_params = { # This dict configures x-axis scale and the data type of th ecorresponding variables\n",
    "    'trans__size' :     {'scale': 'linear'},\n",
    "    'trans__iter':      {'scale': 'linear'},\n",
    "    'trans__alpha':     {'scale': 'log'},\n",
    "    'trans__min_alpha': {'scale': 'log'},\n",
    "    'trans__negative':  {'scale': 'linear'},\n",
    "    'trans__min_count': {'scale': 'linear'},\n",
    "    'trans__window':    {'scale': 'linear'},\n",
    "    'trans__hs':        {'scale': 'linear'},\n",
    "    'trans__dm':        {'scale': 'linear'},\n",
    "    'cls__k':           {'scale': 'linear'},\n",
    "    'sample_size':      {'scale': 'log'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Plotting style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "# colors and fonts\n",
    "plt.style.use('seaborn-paper')\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"paper\")\n",
    "mpl.rc(\"font\", family=\"cmr10\")\n",
    "\n",
    "# sizes\n",
    "plot_width=4.802 # the width of a lncs column\n",
    "plot_height = plot_width / 1.618\n",
    "fontsize = 8\n",
    "\n",
    "# linestyles\n",
    "def linestyle_gen():\n",
    "    colors = [(0, 0, 0), (.6, .6, .6)]\n",
    "    linestyles = ['-', '--', ':']\n",
    "    return product(colors, linestyles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Plot description\n",
    "The following plots depict the results per corpus/sample size manner (that is, each plot deals with a given sample size on one corpus).  \n",
    "Each plot creates an overview of all parameters that were taken into account. The plots are organized in a row-wise manner; elements on the diagonal of these plots contain the legend for all the other plots in that row. Likewise, the labels on the left of the leftmost subplot in a row specify the variable of interest in that row.\n",
    "All y-axes in the plots depict the mean success rate of the classification task. Each subplot describes the influence of the corresponding variables on the mean results of the parameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "for (name, subexperiment), size in product(experiment_data['corpora'].items(), experiment_data['sample_sizes']):\n",
    "    plot_correletaion_matrix(subexperiment['gs_results'],\n",
    "                             experiment_data['gs_params'],\n",
    "                             size, \n",
    "                             '{}@{} Samples'.format(name, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Closer look onto some of the previous plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from itertools import cycle\n",
    "\n",
    "def plot_correlations(experiment_data, corpus, sample_size, params, y_margin=.1):\n",
    "    df = experiment_data['corpora'][corpus]['gs_results']\n",
    "    df = df.loc[df['sample_size'] == sample_size]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(plot_width, plot_height))\n",
    "    \n",
    "    param1, param2, s, t = params\n",
    "        \n",
    "    grouped = df['mean'].groupby([df[param1], df[param2]])\n",
    "    mean = grouped.mean()\n",
    "    std = grouped.mean()\n",
    "    for (instance, series), (c, linestyle) in zip(mean.unstack().iterrows(), cycle(linestyle_gen())):\n",
    "        param1_name = experiment_data['param_labels'][param1]\n",
    "        param2_name = experiment_data['param_labels'][param2]\n",
    "        data = np.array(list(series.items()))\n",
    "        min_max_x = min(data[:, 0]), max(data[:, 0])\n",
    "        min_max_y = (\n",
    "            max(0, min(data[:, 1])*(1-y_margin)), \n",
    "            min(1, max(data[:, 1])*(1+y_margin)))\n",
    "        label = r'$\\theta_{' + param1_name + r'}=' + str(t(instance)) + r'$'\n",
    "\n",
    "        ax.plot(data[:, 0], data[:, 1], linestyle, color=c, label=label)\n",
    "        ax.grid(True)\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_xlabel(r'$\\theta_{' + param2_name + r'}$')\n",
    "        ax.set_xlim(min_max_x)\n",
    "        ax.set_ylim(min_max_y)\n",
    "        ax.set_xscale(s)\n",
    "        ax.legend(loc='upper left')\n",
    "    \n",
    "    param_name = experiment_data['param_labels'][params[0]]\n",
    "    plt.savefig('figures/correlation_{}_{}_{}@{}.eps'\n",
    "        .format(param1, param2, corpus, sample_size), bbox_inches='tight', )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variables = experiment_data['gs_params'].keys()\n",
    "\n",
    "for param1, param2 in product(variables, variables):\n",
    "    if param1 == param2:\n",
    "        continue\n",
    "    params = ((param1, param2, 'linear', float))\n",
    "    plot_correlations(experiment_data, corpus, max_sample_size, params, y_margin=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "source": [
    "# Best scoring results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_dataframe(experiment_data, corpus_name='amazon', sample_size=10000, start=None, end=None):\n",
    "    df = experiment_data['corpora'][corpus_name]['gs_results']\n",
    "    sorted_df = df.sort_values('mean', ascending=False)\n",
    "    return sorted_df.loc[sorted_df['sample_size'] == sample_size].iloc[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = load_dataframe(experiment_data, corpus_name='20newsgroups', sample_size=100000)\n",
    "df.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.iloc[int((len(df)/2)-5):int((len(df)/2)+5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.iloc[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "def plot_best_classifiers(experiment_data,\n",
    "    parameter='trans__size'):\n",
    "\n",
    "    corpora = list(experiment_data['corpora'].keys())\n",
    "    sample_sizes = experiment_data['sample_sizes']\n",
    "    param_name = experiment_data['param_labels'][parameter]\n",
    "    \n",
    "    fig, axis = plt.subplots(2, 2, figsize=(plot_width, plot_height))\n",
    "\n",
    "    for ax, (corpus, sample_size) in zip(axis.flatten(), product(corpora, sample_sizes)):\n",
    "        for instance, (c, s) in zip(experiment_data['gs_params'][parameter], cycle(linestyle_gen())):\n",
    "            df = load_dataframe(experiment_data, corpus_name=corpus, sample_size=sample_size)\n",
    "            df_sub = df.loc[df[parameter] == instance]\n",
    "            num_data = len(df_sub)\n",
    "            x = range(num_data)\n",
    "            label=r'$\\theta_{' + param_name + '}=' + str(instance) + '$'\n",
    "            ax.plot(x, df_sub['mean'], color=c, linestyle=s, label=label)\n",
    "            ax.set_xlabel('Rank', fontsize=fontsize)\n",
    "            ax.set_ylabel('Accuracy', fontsize=fontsize)\n",
    "            ax.set_title('{}, {} training docs'.format(corpus, sample_size), fontsize=fontsize)\n",
    "            ax.set_ylim((0, 1))\n",
    "            ax.set_xticks([int(x) for x in np.arange(0, len(df_sub)+1,  (len(df_sub))/5)])\n",
    "            ax.grid(True)\n",
    "            ax.set_axis_bgcolor('white')\n",
    "\n",
    "    plt.rc('xtick', labelsize=fontsize)\n",
    "    plt.rc('ytick', labelsize=fontsize)\n",
    "    plt.rc('axes', labelsize=fontsize)\n",
    "\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1.1, 2.8))\n",
    "    plt.subplots_adjust(wspace=.35, hspace=.65)\n",
    "\n",
    "    plt.savefig('figures/best_classifiers_{}.eps'.format(param_name), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for param in experiment_data['gs_params']:\n",
    "    plot_best_classifiers(experiment_data, parameter=param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Probabilistic View \n",
    "Let's assume there is a function $f(\\theta) \\mapsto [0, 1]$ (with $\\theta \\in \\mathbb{R}^{n}$) that defines the success rate of our model in the classification task at hand. As this function is unknown we can't examine it analytically.  \n",
    "But by means of the gridsearch that we have carried out, we possess several instances of this function like for example:\n",
    "\n",
    "$$f(\\theta_1=20, \\theta_2=0.1, ..., \\theta_n=100) = 0.659$$.\n",
    "\n",
    "We can gain further insight if we investigate these instances probabilistically. We can chose an arbitrary instance of a variable (i.e. $\\theta_{learning\\,rate}=.25$) and consider all data points that fulfill this constraint as being sampled from:\n",
    "\n",
    "$$f(\\theta \\setminus \\theta_{learning\\,rate} \\, \\mid \\,\\theta_{learning\\,rate}=.25) \\sim \\mathcal{N}(\\mu,\\,\\sigma^{2})$$.\n",
    "\n",
    "Calculating $\\mu$ and $\\sigma^{2}$ allows us to plot and compare for example:  \n",
    "$$f(\\theta \\setminus \\theta_{learning\\,rate} \\, \\mid \\,\\theta_{learning\\,rate}=.25)$$\n",
    "$$f(\\theta \\setminus \\theta_{learning\\,rate} \\, \\mid \\,\\theta_{learning\\,rate}=.1)$$\n",
    "\n",
    "In the following we will try this approach by using the $\\theta_{size}$ parameter which determines the embedding size. We will plot $\\mathcal{N}(\\mu_{\\theta_{size}=2},\\,\\sigma^{2}_{\\theta_{size}=2})$, $\\mathcal{N}(\\mu_{\\theta_{size}=8},\\,\\sigma^{2}_{\\theta_{size}=8})$ and $\\mathcal{N}(\\mu_{\\theta_{size}=24},\\,\\sigma^{2}_{\\theta_{size}=24})$ as well as the histogram of the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "parameter = 'trans__dm'\n",
    "df = load_dataframe(experiment_data, corpus_name='amazon', sample_size=max_sample_size)\n",
    "for instance in experiment_data['gs_params'][parameter]:\n",
    "    sub_df = df.loc[df[parameter] == instance]\n",
    "    data = sorted(sub_df['mean'])\n",
    "    fit = stats.norm.pdf(data, np.mean(data), np.std(data))\n",
    "    plt.plot(data, fit, '-', label=r'$\\mathcal{N}_{size}=' + str(instance) + '$')\n",
    "    plt.hist(data, alpha=.2, normed=True)\n",
    "\n",
    "plt.yticks([])\n",
    "plt.legend(loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "From this plot we can see two interesting aspects:\n",
    "1. We can see that 8 and 24 dimensional embeddings on average perform better than 2 dimensional ones.\n",
    "2. The assumption that the sampled data is normally distributed is inapplicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gaussian Kernel Density Estimation\n",
    "By means of Kernel Density Estimation(KDE) we can obtain a distribution that models the observed data much better than the normal distributions in the previous section. The following plot depict the distributions probability density functions received from KDE. Comparing those to the histogram in the previous plot, it can be seen that they resemble the underlying data much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.kde import gaussian_kde\n",
    "from numpy import linspace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def p_given(experiment_data, parameter, corpus_name='amazon', sample_size=10000):\n",
    "    df = load_dataframe(experiment_data, corpus_name=corpus_name, sample_size=sample_size)\n",
    "    values = experiment_data['gs_params'][parameter]\n",
    "    param_name = experiment_data['param_labels'][parameter]\n",
    "    \n",
    "    # sort the data\n",
    "    sorted_data = []\n",
    "    for value in values:\n",
    "        sorted_data.append(sorted(df[df[parameter] == value]['mean']))\n",
    "\n",
    "    # kernel density esimation\n",
    "    KDEs = []\n",
    "    for succ_rate in sorted_data:\n",
    "        KDEs.append(gaussian_kde(succ_rate))\n",
    "\n",
    "    min_val = df['mean'].min()\n",
    "    max_val = df['mean'].max()\n",
    "    x = linspace(min_val, max_val, 100)\n",
    "\n",
    "    num_data = len(sorted_data[0])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(plot_width, plot_height)\n",
    "    \n",
    "    for kde, (c, style), label in zip(KDEs, cycle(linestyle_gen()), values):\n",
    "        label = r'$P(\\theta \\: | \\: \\theta_{' + param_name + '}=' + str(label) + ')$'\n",
    "        ax.plot(x, kde(x)/num_data, color=c, linestyle=style, label=label)\n",
    "\n",
    "    plt.rc('xtick', labelsize=fontsize)\n",
    "    plt.rc('ytick', labelsize=fontsize)\n",
    "    plt.rc('axes', labelsize=fontsize)\n",
    "\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(False)\n",
    "    plt.xlabel('Accuracy', fontsize=fontsize)\n",
    "    plt.ylabel('Probability', fontsize=fontsize)\n",
    "    plt.xlim((min_val, max_val))\n",
    "\n",
    "    plt.savefig('figures/PDF_{}_{}@{}.eps'.format(param_name, corpus, sample_size), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for parameter in experiment_data['gs_params'].keys():\n",
    "    p_given(experiment_data, parameter, corpus_name=corpus, sample_size=max_sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The vast majority of the probability mass of $KDE(\\theta_{size}=2)$ is between $0.6$ and $0.8$. in contrast $KDE(\\theta_{size}=8)$ and $KDE(\\theta_{size}=24)$ perform better. But from this graph it's difficult to decide which of these is actually better. $KDE(\\theta_{size}=8)$ has more probability mass between $0.7$ and $0.8$ but peaks higher at around $0.9$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
